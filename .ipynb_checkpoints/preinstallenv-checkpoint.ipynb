{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this all about?\n",
    "\n",
    "In order to run Spark applications on your local machine, you must have **Java 8**, **Spark**, and the **PySpark** package installed.  Additionally, for the Jupyter Notebook kernel to use your locally installed **Spark**, you may need to use the **findspark** module.   If you are unsure whether you have any or all of these requirements, we recommend you follow the instructions in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Java Installation\n",
    "\n",
    "Executing the following cell will install **Java 8** into your home directory (i.e., `$HOME`) in its own directory (i.e., `$HOME/java/`).  If you would rather install Java yourself, do not execute the following cell and download Java and follow the instructions for your operating system found on the [Java website](https://www.java.com/en/download/).\n",
    "\n",
    "*Note: If you have Java already installed, we recommend uninstalling it before running the following cell.  The next cell of code will delete anything located in `$HOME/java/` and add the installed Java directory to your `PATH` variable.  This installation code also assumes you use Bash as your default shell (and will modify your `$HOME/.bashrc` file).*\n",
    "\n",
    "**OS specific notes:**\n",
    "\n",
    "*Linux distros: This code has been tested and works for most distributions of Linux (32 and 64 bit)*\n",
    "\n",
    "*Mac OSX: The code to install Java may or may not work on your machine.  If you experience an error, please download Java and follow instructions for installation found on the [Java website](https://www.java.com/en/download/).*\n",
    "\n",
    "*Windows OS: You must download and follow instructions for installation found on the [Java website](https://www.java.com/en/download/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Java 8 installation!\n",
      "Now installing Java on Linux...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   909    0   909    0     0   3223      0 --:--:-- --:--:-- --:--:--  3223\n",
      "100 77.4M  100 77.4M    0     0  3133k      0  0:00:25  0:00:25 --:--:-- 2999k\n",
      "Installation of Java 8 complete!\n"
     ]
    }
   ],
   "source": [
    "import platform as arch\n",
    "from sys import platform\n",
    "   \n",
    "print('Beginning Java 8 installation!')\n",
    "\n",
    "# Check which OS we are running on\n",
    "if platform.startswith('linux'):\n",
    "    print('Now installing Java on Linux...')\n",
    "    if arch.architecture()[0] == '64bit':\n",
    "        !curl -o ~/java.tar.gz -L http://javadl.oracle.com/webapps/download/AutoDL?BundleId=234464_96a7b8442fe848ef90c96a2fad6ed6d1\n",
    "    elif arch.architecture()[0] == '32bit':\n",
    "        !curl -o ~/java.tar.gz -L http://javadl.oracle.com/webapps/download/AutoDL?BundleId=234462_96a7b8442fe848ef90c96a2fad6ed6d1\n",
    "    !rm -rf ~/java && mkdir ~/java && tar -xzf ~/java.tar.gz -C ~/java --strip-components=1 && rm ~/java.tar.gz\n",
    "\n",
    "    # Define JAVA_HOME and add to PATH\n",
    "    !echo 'export JAVA_HOME=$HOME/java' >> ~/.bashrc\n",
    "    !echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc\n",
    "    !. ~/.bashrc\n",
    "    \n",
    "    print('Installation of Java 8 complete!')\n",
    "\n",
    "elif platform == 'darwin':\n",
    "    print('Now installing Java on Mac...')\n",
    "    !curl -o ~/java.dmg -L http://javadl.oracle.com/webapps/download/AutoDL?BundleId=234465_96a7b8442fe848ef90c96a2fad6ed6d1\n",
    "    !hdiutil attach ~/java.dmg\n",
    "    !sudo installer -pkg /Volumes/Java\\ 8\\ Update\\ 181/Java\\ 8\\ Update\\ 181.app/Contents/Resources/JavaAppletPlugin.pkg -target /\n",
    "    !diskutil umount /Volumes/Java\\ 8\\ Update\\ 181 \n",
    "    print('Installation of Java 8 complete (maybe)... If there was an error, please mount java.dmg located in your home directory and follow the instructions to install')\n",
    "\n",
    "elif platform == 'win32':\n",
    "    print('You are running a Windows OS.  Please download the correct version of Java from here: https://java.com/en/download/manual.jsp and install following the instructions.')\n",
    "\n",
    "else:\n",
    "    print('We had trouble determining which OS you are running.  Please ask for help.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Spark Installation\n",
    "\n",
    "Executing the following cell will install the latest **Apache Spark** into your home directory (i.e., `$HOME`) in its own directory (i.e., `$HOME/spark/`).\n",
    "\n",
    "If you would rather install Spark yourself, do not execute the following cell and download the pre-built version of Spark found on the [Spark website](https://spark.apache.org/downloads.html).  You will need to extract the contents of the tarball and add Spark to your `PATH` variable.\n",
    "\n",
    "*Note: If you have Spark already installed, we recommend uninstalling it before running the following cell.  The next cell of code will delete anything located in `$HOME/spark/` and add the installed Spark directory to your `PATH` variable.  This installation code also assumes you use Bash as your default shell (and will modify your `$HOME/.bashrc` file).*\n",
    "\n",
    "**OS specific notes:**\n",
    "\n",
    "*Linux distros: This code has been tested and works for most distributions of Linux (32 and 64 bit).*\n",
    "\n",
    "*Mac OSX: The code has been tested and should work for modern Macs.*\n",
    "\n",
    "*Windows OS: You must download and follow instructions for installation found on the [Spark website](https://spark.apache.org/downloads.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Apache Spark installation!\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      " 22  215M   22 47.6M    0     0  1209k      0  0:03:02  0:00:40  0:02:22 1207k"
     ]
    }
   ],
   "source": [
    "from sys import platform\n",
    "\n",
    "print('Beginning Apache Spark installation!')\n",
    "\n",
    "# Check which OS we are running on\n",
    "if (platform.startswith('linux')) or (platform == 'darwin'):\n",
    "    # Download Spark and extract into $HOME/spark/\n",
    "    !curl -o ~/spark.tar.gz -L http://apache.cs.utah.edu/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz\n",
    "    !rm -rf ~/spark && mkdir ~/spark && tar -xzf ~/spark.tar.gz -C ~/spark --strip-components=1 && rm ~/spark.tar.gz\n",
    "    \n",
    "    # Define SPARK_HOME and add to PATH\n",
    "    !echo 'export SPARK_HOME=$HOME/spark' >> ~/.bashrc\n",
    "    !echo 'export PATH=$SPARK_HOME/bin:$PATH' >> ~/.bashrc\n",
    "    \n",
    "    # Set spark master to localhost (may not be necessary)\n",
    "    !echo 'export SPARK_LOCAL_IP=\"127.0.0.1\"' >> ~/.bashrc\n",
    "    !. ~/.bashrc\n",
    "    \n",
    "    print('Installation of Apache Spark complete!')\n",
    "\n",
    "elif platform == 'win32':\n",
    "    print('You are running a Windows OS.  Please download the correct version of Spark from here: https://spark.apache.org/downloads.html and install following the instructions.')\n",
    "\n",
    "else:\n",
    "    print('We had trouble determining which OS you are running.  Please ask for help.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Pyspark Installation\n",
    "\n",
    "Executing the following cell will install **pyspark** and **findspark**.  We will use either `conda` (if you have anaconda3 or miniconda installed) or `pip`.  At the minimum, you must have `pip` installed.  You likely have one of these installed already!  If you do not, you can download and install [Anaconda3](https://www.anaconda.com/download/) or [pip](https://pip.pypa.io/en/stable/installing/).  We will also install **matplotlib** so you can plot results from your assignments and pre-build the font cache to save time in the future!\n",
    "\n",
    "*Note: These instructions should work regardless of the OS you are running!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Method to build Matplotlib font cache\n",
    "def buildFontCache():\n",
    "    import matplotlib\n",
    "    matplotlib.use('AGG')\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.plot([0],[0])\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "# Check for conda\n",
    "if shutil.which('conda'):\n",
    "    # Update conda\n",
    "    !conda update -n base conda --yes\n",
    "    \n",
    "    # Install pyspark and findspark\n",
    "    !conda install pyspark --yes\n",
    "    !conda install -c conda-forge findspark --yes\n",
    "    \n",
    "    # Install matplotlib and build font cache\n",
    "    !conda install matplotlib --yes\n",
    "    buildFontCache()\n",
    "    \n",
    "    print('Python package installation complete!')\n",
    "\n",
    "    \n",
    "# Check for pip if conda is not found\n",
    "elif shutil.which('pip'):\n",
    "    # Update pip\n",
    "    !pip install --upgrade pip\n",
    "    \n",
    "    # Install pyspark and findspark\n",
    "    !pip install pyspark\n",
    "    !pip install findspark\n",
    "    \n",
    "    # Install matplotlib and build font cache\n",
    "    !pip install matplotlib\n",
    "    buildFontCache()\n",
    "    \n",
    "    print('Python packages installation complete!')\n",
    "    \n",
    "else:\n",
    "    print('Could not find conda or pip, please follow the instructions above to install either Anaconda3 or pip.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it Work?\n",
    "\n",
    "Let's test that **Java**, **Spark**, **pyspark**, and **findspark** were all installed correctly.  The following should create a `SparkContext`, create an `RDD` from a python list, and print the values in the `RDD`.\n",
    "\n",
    "**Expected output:**\n",
    "`[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "data = sc.parallelize(range(10))\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the output of the previous cell did not match the expected output or you received an error message, please ask for assistance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Spark Code\n",
    "\n",
    "Now that you have all the software installed to run **Spark** code in a Jupyter Notebook, keep in mind that you will need to use the following code at the beginning of each notebook where you wish to use **Spark**.  This initialization code will create a `SparkContext` which you can access via `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov 20 20:21:31 2018\n",
    "\n",
    "@author: trahman4\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "dataset_tags = pd.read_csv('scav_mumm.csv').iloc[:,:-1]\n",
    "dataset_max_min = pd.read_csv('outputs/maxdate_mindate.csv')\n",
    "dataset_weather = pd.read_csv('KnoxvilleWeather_modified.csv')\n",
    "\n",
    "scav_df = dataset_tags[dataset_tags['tag'].str.contains('scav')]\n",
    "mumm_df = dataset_tags[dataset_tags['tag'].str.contains('mumm')]\n",
    "\n",
    "dataset_weather[\"date\"] = \"\"\n",
    "\n",
    "#convert the weather dataset's date to date object date so that we can use the match to find date range\n",
    "dataset_weather['date']= dataset_weather['dt_iso'].apply(lambda x: \n",
    "                    datetime.datetime(int(x.split(\" \")[0].split(\"-\")[0]),int(x.split(\" \")[0].split(\"-\")[1]),int(x.split(\" \")[0].split(\"-\")[2])).date())\n",
    "\n",
    "\n",
    "donor_match_df = pd.DataFrame\n",
    "match = pd.DataFrame\n",
    "\n",
    "\n",
    "#this function will take a dataframe of certain tags and will return the min date \n",
    "#of all the minimum dates of each images and max dates for all maximium...\n",
    "#meaning each image has a max(current) date and a minimum date...we collect all the minimum dates for all images\n",
    "#same for max\n",
    "def collect_max_min_date(tag, dataf):\n",
    "    global donor_match_df,match\n",
    "    print(\"We are working on \", tag , \" tag\")\n",
    "    min_dates = []\n",
    "    max_dates = []\n",
    "    donor = choose_donor(dataf)\n",
    "    print(\"The donor with most data is \", donor)\n",
    "    donor_match_df = dataf[dataf['image'].str.contains(donor)] #matching the image name with donor's name\n",
    "    \n",
    "    for row in donor_match_df.iterrows():\n",
    "        image_name = row[1].values[0]\n",
    "        image_name = image_name.split(\" \")[0]      \n",
    "        match = dataset_max_min[dataset_max_min['image_name'].str.match(image_name)] #matching the image name with tag's data with max_min_date data\n",
    "        \n",
    "        max_dates.append(match['date_from_image/max_date'].values[0])\n",
    "        min_dates.append(match['min_date/first date'].values[0])\n",
    "    max_date = max(max_dates)\n",
    "    min_date = min(min_dates)\n",
    "    return max_date, min_date\n",
    "\n",
    "\n",
    "\n",
    "def choose_donor(dataf):\n",
    "    donors = []\n",
    "    for row in dataf.iterrows():\n",
    "        donor_name = row[1].values[0].split(\" \")[0].split(\"_\")[0]\n",
    "        donors.append(donor_name)\n",
    "    freq_rdd = sc.parallelize(donors)\n",
    "\n",
    "    freq_rdd_count = freq_rdd.map(makeKeyValue)\n",
    "    freq_rdd_sum = freq_rdd_count.reduceByKey(addValues)\n",
    "    \n",
    "    freq_rdd_counts_sorted = sorted (freq_rdd_sum.collect(), key = lambda x: x[1], reverse = True)\n",
    "    print_map_reduce_data(freq_rdd_counts_sorted)\n",
    "    return freq_rdd_counts_sorted[1][0]\n",
    "\n",
    "#this function gets a string in the form of \"YYYY-MM-DAY\" and return date object    \n",
    "def make_date(curr_date):\n",
    "    year, month, day = str(curr_date).split(\"-\")            \n",
    "    curr_date = date(int(year), int(month), int(day))\n",
    "    return curr_date    \n",
    "            \n",
    "\n",
    "def makeKeyValue(key, value=1):\n",
    "    return (key, value)\n",
    "\n",
    "# Count (reduce) the values for a given key (word length)\n",
    "def addValues(val1, val2):\n",
    "    return val1 + val2\n",
    "\n",
    "def plot_histo(tag,sorted_list,x_label,y_label,title):\n",
    "    plt.clf()\n",
    "    x, y = zip(*sorted_list)\n",
    "    fig, axs = plt.subplots(1,1,figsize=(20,9))\n",
    "    index = np.arange(len(x))\n",
    "    plt.barh(index, y, color ='g')\n",
    "    plt.xlabel(y_label, fontsize=5)\n",
    "    plt.ylabel(x_label, fontsize=10)\n",
    "    plt.yticks(index, x, fontsize=10, rotation=30)\n",
    "    plt.title(title)\n",
    "    plt.savefig('plt_'+str(tag)+'.png', dpi = 300)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def print_map_reduce_data(sorted_list):  \n",
    "    print('Weather Type          :  Count')\n",
    "    for weather_f, count in sorted_list:\n",
    "        print('{:<20}    : {:>6}'.format(weather_f, count))\n",
    "        \n",
    "    ls = [vehicle_body for vehicle_body, count in sorted_list[:]]\n",
    "        \n",
    "    print (\"Total Number of Weather Types: \", len(ls))\n",
    "    \n",
    "    \n",
    "    \n",
    "def map_reduce(tag, list_to_mr):\n",
    "    weather_freq = sc.parallelize(list_to_mr)\n",
    "\n",
    "    weather_freq_count = weather_freq.map(makeKeyValue)\n",
    "    weather_freq_sum = weather_freq_count.reduceByKey(addValues)\n",
    "    \n",
    "    weather_freq_counts_sorted = sorted (weather_freq_sum.collect(), key = lambda x: x[1], reverse = True)\n",
    "    print_map_reduce_data(weather_freq_counts_sorted)\n",
    "    plot_histo(tag, weather_freq_counts_sorted, \"type\", \"freq\", \"histogram for \"+str(tag))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_normal_distribution(temp, column_name):\n",
    "    \n",
    "    mu, std = norm.fit(temp)\n",
    "    \n",
    "    plt.hist(temp, bins=25, density=True, alpha=0.6, color='g')\n",
    "    \n",
    "    # Plot the PDF.\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2)\n",
    "    title = \"Normal Distribution of %s\\nFit results: mu = %.2f,  std = %.2f\" % (column_name,\n",
    "                                                                                mu, std)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "      \n",
    "scav_max_date, scav_min_date = collect_max_min_date(\"scav\",scav_df)\n",
    "scav_min_date = make_date(scav_min_date)\n",
    "scav_max_date = make_date(scav_max_date)\n",
    "\n",
    "\n",
    "mumm_max_date, mumm_min_date = collect_max_min_date(\"mumm\",mumm_df)\n",
    "mumm_min_date = make_date(mumm_min_date)\n",
    "mumm_max_date = make_date(mumm_max_date)\n",
    "\n",
    "#getting the date range between max date and min date\n",
    "scav_date_range = [scav_min_date + timedelta(days=x) for x in range((scav_max_date-scav_min_date).days + 1)]\n",
    "mumm_date_range = [mumm_min_date + timedelta(days=x) for x in range((mumm_max_date-mumm_min_date).days + 1)]\n",
    "\n",
    "#extract the weather dataset's dates that are within the date range\n",
    "scav_weather_df = dataset_weather[dataset_weather['date'].isin(scav_date_range)]\n",
    "mumm_weather_df = dataset_weather[dataset_weather['date'].isin(mumm_date_range)]\n",
    "\n",
    "#getting the weather description column to build the histogram/mapreduce\n",
    "scav_weather_description = scav_weather_df['weather_description'].values.T.tolist()\n",
    "mumm_weather_description = mumm_weather_df['weather_description'].values.T.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "map_reduce(\"mumm\", mumm_weather_description)\n",
    "map_reduce(\"scav\", scav_weather_description)\n",
    "\n",
    "#plot_normal_distribution(scav_weather_df['humidity'],'scav weather humidity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
